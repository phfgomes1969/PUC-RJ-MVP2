{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phfgomes1969/PUC-RJ-MVP2/blob/main/MVP_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqZRIMSIkypV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "#from tqdm import tqdm\n",
        "\n",
        "!pip install ultralytics -q\n",
        "\n",
        "from ultralytics import YOLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-O9IV_cbFB6",
        "outputId": "b3f51b23-fadb-403c-e2c7-dade99723d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório '/content/dataset/images/train' apagado com sucesso.\n",
            "Diretório '/content/dataset/images/val' apagado com sucesso.\n",
            "Diretório '/content/dataset/labels/train' apagado com sucesso.\n",
            "Diretório '/content/dataset/labels/val' apagado com sucesso.\n",
            "Diretório '/content/dataset' apagado com sucesso.\n",
            "Diretório '/content/runs' não encontrado.\n",
            "Diretório '/content/Fotos_Teste' não encontrado.\n",
            "Diretório '/content/dataset' não encontrado.\n",
            "Diretório '/content/treinado' não encontrado.\n"
          ]
        }
      ],
      "source": [
        "diretorios_para_apagar = [\n",
        "    '/content/dataset/images/train',\n",
        "    '/content/dataset/images/val',\n",
        "    '/content/dataset/labels/train',\n",
        "    '/content/dataset/labels/val',\n",
        "    '/content/dataset',\n",
        "    '/content/runs',\n",
        "    '/content/Fotos_Teste',\n",
        "    '/content/dataset',\n",
        "    '/content/treinado']\n",
        "\n",
        "\n",
        "for diretorio in diretorios_para_apagar:\n",
        "    if os.path.exists(diretorio):  # Verifica se o diretório existe\n",
        "        try:\n",
        "            shutil.rmtree(diretorio)\n",
        "            print(f\"Diretório '{diretorio}' apagado com sucesso.\")\n",
        "        except OSError as e:\n",
        "            print(f\"Erro ao apagar o diretório '{diretorio}': {e}\")\n",
        "    else:\n",
        "        print(f\"Diretório '{diretorio}' não encontrado.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "opwLX4TgWqYT"
      },
      "outputs": [],
      "source": [
        "# Busca os arquivos que compoem o dataset no GitHub\n",
        "# Tive que quebrar em vários pedaços poir limitação do GitHub\n",
        "\n",
        "repo_url = 'https://api.github.com/repos/phfgomes1969/PUC-RJ-MVP2/contents/'\n",
        "\n",
        "# Pasta de destino no Google Colab\n",
        "destino = '/content/dataset'\n",
        "\n",
        "# Cria a pasta de destino se ela não existir\n",
        "os.makedirs(destino, exist_ok=True)\n",
        "\n",
        "# Obtém a lista de arquivos do repositório em formato JSON\n",
        "resposta = requests.get(repo_url)\n",
        "resposta.raise_for_status()\n",
        "conteudo = resposta.json()\n",
        "\n",
        "# Filtra os arquivos com extensão .zip\n",
        "arquivos_zip = [arquivo['name'] for arquivo in conteudo if arquivo['name'].endswith('.zip')]\n",
        "\n",
        "# Baixa e extrai cada arquivo .zip\n",
        "for nome_arquivo in arquivos_zip:\n",
        "    # Constrói a URL completa do arquivo\n",
        "    url_arquivo = f\"https://raw.githubusercontent.com/phfgomes1969/PUC-RJ-MVP2/main/{nome_arquivo}\"\n",
        "\n",
        "    # Baixa o arquivo usando requests\n",
        "    resposta = requests.get(url_arquivo, stream=True)\n",
        "    resposta.raise_for_status()\n",
        "\n",
        "    # Salva o arquivo no disco\n",
        "    caminho_arquivo = os.path.join(destino, nome_arquivo)\n",
        "    with open(caminho_arquivo, 'wb') as arquivo:\n",
        "        for pedaço in resposta.iter_content(chunk_size=8192):\n",
        "            arquivo.write(pedaço)\n",
        "\n",
        "    # Extrai o arquivo ZIP para a pasta de destino\n",
        "    with zipfile.ZipFile(caminho_arquivo, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destino)\n",
        "\n",
        "    # Remove o arquivo .zip\n",
        "    os.remove(caminho_arquivo)\n",
        "    print(f\"Arquivo '{nome_arquivo}' removido.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEZ1PSeLT3z4"
      },
      "outputs": [],
      "source": [
        "# Move todos os arquivos para a pasta de destino e remove subpastas, exceto Fotos_Teste e Treinado\n",
        "dir_excecao = ['Fotos_Teste','Treinado']\n",
        "for root, dirs, files in os.walk(destino, topdown=False):\n",
        "    for name in files:\n",
        "        if not any(exc in root for exc in dir_excecao):   # Ignora Fotos_Teste e treinado\n",
        "            origem = os.path.join(root, name)\n",
        "            destino_final = os.path.join(destino, name)\n",
        "\n",
        "            # Se o arquivo já existe no destino, adiciona um sufixo para evitar conflitos\n",
        "            if os.path.exists(destino_final):\n",
        "                base, ext = os.path.splitext(name)\n",
        "                i = 1\n",
        "                while os.path.exists(os.path.join(destino, f\"{base}_{i}{ext}\")):\n",
        "                    i += 1\n",
        "                destino_final = os.path.join(destino, f\"{base}_{i}{ext}\")\n",
        "\n",
        "            shutil.move(origem, destino_final)\n",
        "\n",
        "    for name in dirs:\n",
        "        if name != destino.split('/')[-1] and name != 'Fotos_Teste' and name != 'Treinado' and not root.startswith(os.path.join(destino, 'Treinado')):\n",
        "            shutil.rmtree(os.path.join(root, name))\n",
        "            print(f\"Pasta '{name}' removida.\")\n",
        "\n",
        "print(\"Processo concluído. Todos os arquivos extraídos e organizados em 'content/dataset'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VVBd44-44BC"
      },
      "outputs": [],
      "source": [
        "# 1. Definir os caminhos\n",
        "dataset_dir = '/content/dataset'  # Diretório principal do dataset\n",
        "images_dir = os.path.join(dataset_dir, 'images')\n",
        "labels_dir = os.path.join(dataset_dir, 'labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZMPKy3A4393"
      },
      "outputs": [],
      "source": [
        "# 2. Criar as pastas (se não existirem)\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "os.makedirs(labels_dir, exist_ok=True)\n",
        "os.makedirs(os.path.join(images_dir, 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(images_dir, 'val'), exist_ok=True)\n",
        "os.makedirs(os.path.join(labels_dir, 'train'), exist_ok=True)\n",
        "os.makedirs(os.path.join(labels_dir, 'val'), exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zpt4vGbN436n"
      },
      "outputs": [],
      "source": [
        "# 3. Obter todos os arquivos de imagem e anotação, ignorando Fotos_Teste\n",
        "all_files = glob.glob(os.path.join(dataset_dir, '**', '*'), recursive=True)\n",
        "all_images = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg', '.png')) and 'Fotos_Teste' not in f and 'Treinado' not in f]  # Filtra imagens e ignora Fotos_Teste\n",
        "all_annotations = [f for f in all_files if f.lower().endswith('.csv') and 'Fotos_Teste' not in f and 'Treinado' not in f]  # Filtra anotações e ignora Fotos_Teste\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDnguxw6433S"
      },
      "outputs": [],
      "source": [
        "# 4. Dividir os dados em treino e validação (80% treino, 20% validação)\n",
        "num_images = len(all_images)\n",
        "num_train = int(num_images * 0.8)\n",
        "\n",
        "train_images = all_images[:num_train]\n",
        "val_images = all_images[num_train:]\n",
        "train_annotations = all_annotations[:num_train]\n",
        "val_annotations = all_annotations[num_train:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAwwiiI8430M"
      },
      "outputs": [],
      "source": [
        "# 5. Mover os arquivos para as pastas corretas\n",
        "for image_path in train_images:\n",
        "    shutil.move(image_path, os.path.join(images_dir, 'train'))\n",
        "for annotation_path in train_annotations:\n",
        "    shutil.move(annotation_path, os.path.join(labels_dir, 'train'))\n",
        "for image_path in val_images:\n",
        "    shutil.move(image_path, os.path.join(images_dir, 'val'))\n",
        "for annotation_path in val_annotations:\n",
        "    shutil.move(annotation_path, os.path.join(labels_dir, 'val'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdu8ZjHTOxFD"
      },
      "outputs": [],
      "source": [
        "# Renomear os arquivos CSV para TXT\n",
        "def convert_csv_to_yolo(csv_file, txt_file, classes):\n",
        "    \"\"\"Converte um arquivo CSV de anotações para o formato TXT do YOLO.\n",
        "\n",
        "    Args:\n",
        "        csv_file: Caminho para o arquivo CSV.\n",
        "        txt_file: Caminho para o arquivo TXT de saída.\n",
        "        classes: Lista de nomes das classes.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(csv_file)\n",
        "    with open(txt_file, 'w') as f:\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            # Assumindo que as colunas do CSV são: filename, width, height, class, xmin, ymin, xmax, ymax\n",
        "            filename = row['filename']\n",
        "            width = row['width']\n",
        "            height = row['height']\n",
        "            class_name = row['class']\n",
        "            xmin = row['xmin']\n",
        "            ymin = row['ymin']\n",
        "            xmax = row['xmax']\n",
        "            ymax = row['ymax']\n",
        "\n",
        "            # Calcular as coordenadas normalizadas do centro, largura e altura\n",
        "            x_center = (xmin + xmax) / (2 * width)\n",
        "            y_center = (ymin + ymax) / (2 * height)\n",
        "            bbox_width = (xmax - xmin) / width\n",
        "            bbox_height = (ymax - ymin) / height\n",
        "\n",
        "            # Converter classes para uma lista se for um array NumPy\n",
        "            if isinstance(classes, np.ndarray):\n",
        "                classes = classes.tolist()  # Converter para lista para usar append\n",
        "\n",
        "            # Verificar se a classe está na lista e adicioná-la se não estiver\n",
        "            if class_name not in classes:\n",
        "                classes.append(class_name)\n",
        "\n",
        "\n",
        "            # Obter o índice da classe\n",
        "            class_index = classes.index(class_name)\n",
        "\n",
        "            # Escrever a linha no formato YOLO\n",
        "            f.write(f\"{class_index} {x_center} {y_center} {bbox_width} {bbox_height}\\n\")\n",
        "\n",
        "\n",
        "    return classes  # Retornar a lista classes modificada\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HcKaczKOMmhA"
      },
      "outputs": [],
      "source": [
        "# Obter todos os arquivos CSV\n",
        "csv_files = glob.glob(os.path.join(labels_dir, '**', '*.csv'), recursive=True)\n",
        "\n",
        "# Cria a variavel classes vazia\n",
        "classes = np.array([])\n",
        "\n",
        "# Iterar pelos arquivos CSV e convertê-los\n",
        "for csv_file in csv_files:\n",
        "    # Criar o caminho para o arquivo TXT correspondente\n",
        "    txt_file = csv_file.replace('.csv', '.txt')#.replace(labels_dir, images_dir)\n",
        "    txt_file = os.path.splitext(txt_file)[0] + '.txt'\n",
        "\n",
        "    # Criar a pasta de destino se não existir\n",
        "    os.makedirs(os.path.dirname(txt_file), exist_ok=True)\n",
        "\n",
        "    # Converter o arquivo CSV para TXT padrão YOLO e\n",
        "    # identificar todas as classes (aviões) que estão dentro dos CSV´s\n",
        "    classes = convert_csv_to_yolo(csv_file, txt_file, classes)\n",
        "\n",
        "    #Remove os arquivos CSV´s deixando apenas os TXT´s gerados\n",
        "    os.remove(csv_file)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLROd28L43uL",
        "outputId": "1452d576-618b-41bb-e514-7b45b9eec467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: /content/dataset/images/train\n",
            "val: /content/dataset/images/val\n",
            "nc: 34\n",
            "names: [\"A10\", \"KC135\", \"C130\", \"F35\", \"A400M\", \"Rafale\", \"B1\", \"F16\", \"F15\", \"F18\", \"EF2000\", \"F22\", \"Tornado\", \"Su34\", \"Su25\", \"B52\", \"F4\", \"C2\", \"C5\", \"JAS39\", \"F117\", \"C17\", \"V22\", \"Mirage2000\", \"CH47\", \"AV8B\", \"E7\", \"C390\", \"UH60\", \"U2\", \"JF17\", \"AH64\", \"P3\", \"B2\"]\n"
          ]
        }
      ],
      "source": [
        "# 6. Criar o arquivo data.yaml\n",
        "classes_string = ', '.join([f'\"{c}\"' for c in classes])\n",
        "with open('/content/dataset/data.yaml', mode='w') as f:\n",
        "    yaml_string='train: /content/dataset/images/train\\n'\\\n",
        "              + 'val: /content/dataset/images/val\\n'\\\n",
        "              + f'nc: {len(classes)}\\n'\\\n",
        "              + f'names: [{classes_string}]'\n",
        "    f.write(yaml_string)\n",
        "print(yaml_string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEQ8Q-3j43k4"
      },
      "outputs": [],
      "source": [
        "# 7. Treinando o Modelo\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Definindo os hiperparametros\n",
        "hip = {\n",
        "    'epochs': 2,# O ideal seria acima de 100. Já foi realizado um aprendizado com 100 epochs. Os testes com modelo treinado está mais abaixo\n",
        "    'batch': 16,\n",
        "    'imgsz': 640,# testar com 416\n",
        "    'lr0': 0.005,\n",
        "    'lrf': 0.0005,\n",
        "    'momentum': 0.937,\n",
        "    'weight_decay': 0.0005,\n",
        "    'optimizer': 'SGD'}\n",
        "\n",
        "results = model.train(data='/content/dataset/data.yaml', **hip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UO8vWmiPSHu"
      },
      "outputs": [],
      "source": [
        "\n",
        "for path in sorted(glob.glob('/content/runs/detect/train/*.png')):\n",
        "    image = cv2.imread(path)[:,:,::-1]\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiDYCsaR43hg"
      },
      "outputs": [],
      "source": [
        "for path in sorted(glob.glob('/content/runs/detect/train/*.jpg')):\n",
        "    image = cv2.imread(path)[:,:,::-1]\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU6VcnMYZNHk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4. Processar e exibir os resultados\n",
        "def processar_imagem(results,image,model):\n",
        "    for r in results:\n",
        "\n",
        "        # Obter as caixas delimitadoras e rótulos das detecções\n",
        "        boxes = r.boxes\n",
        "        xyxy = boxes.xyxy.cpu().numpy()  # Coordenadas das caixas delimitadoras\n",
        "        conf = boxes.conf.cpu().numpy()  # Confiança das detecções\n",
        "        cls = boxes.cls.cpu().numpy()  # Classes das detecções\n",
        "\n",
        "        # Desenhar as caixas delimitadoras na imagem original\n",
        "        for i in range(len(boxes)):\n",
        "            x1, y1, x2, y2 = map(int, xyxy[i])\n",
        "            label = f'{model.names[int(cls[i])]} {conf[i]:.2f}'\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Desenhar retângulo verde\n",
        "            cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)  # Escrever rótulo\n",
        "\n",
        "        # 5. Exibir a imagem com as detecções\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Converter BGR para RGB para o Matplotlib\n",
        "        plt.axis('off')  # Remover eixos\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-cw2FAc8Gn7"
      },
      "outputs": [],
      "source": [
        "# Validar aprendizado com novas figuras\n",
        "\n",
        "def testar_imagens_jpg(diretorio,model):\n",
        "\n",
        "  \"\"\"\n",
        "  Varre um diretório e testa todos os arquivos JPG encontrados.\n",
        "\n",
        "  Args:\n",
        "    diretorio: O caminho para o diretório a ser varrido.\n",
        "  \"\"\"\n",
        "\n",
        "  # 2. Obter lista de todos os arquivos JPG no diretório\n",
        "  arquivos_jpg = glob.glob(os.path.join(diretorio, '**', '*.jpg'), recursive=True)\n",
        "\n",
        "  # 2. Iterar sobre os arquivos e realizar o teste\n",
        "  for arquivo_jpg in arquivos_jpg:\n",
        "    print(f\"Testando imagem: {arquivo_jpg}\")\n",
        "\n",
        "    # 3. Realizar a inferência\n",
        "    image = cv2.imread(arquivo_jpg)\n",
        "    results = model(arquivo_jpg)\n",
        "    processar_imagem(results,image,model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oho0z0PC8Geu"
      },
      "outputs": [],
      "source": [
        "# Testar o aprendizado com imagens diferentes\n",
        "\n",
        "diretorio = '/content/dataset/Fotos_Teste'\n",
        "\n",
        "# 1. Carregar o modelo treinado\n",
        "model = YOLO('/content/runs/detect/train/weights/best.pt')\n",
        "\n",
        "testar_imagens_jpg(diretorio,model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x531W-Fa57Xu"
      },
      "source": [
        "***ABAIXO, SEGUE O MESMO TESTE COM UM MODELO TREINADO (100 EPOCHs)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4cGN8d281p_"
      },
      "outputs": [],
      "source": [
        "for path in sorted(glob.glob('/content/dataset/Treinado/*.png')):\n",
        "    image = cv2.imread(path)[:,:,::-1]\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqVN94LL9bZ6"
      },
      "outputs": [],
      "source": [
        "for path in sorted(glob.glob('/content/dataset/Treinado/*.jpg')):\n",
        "    image = cv2.imread(path)[:,:,::-1]\n",
        "    plt.figure(figsize=(20,20))\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRydI7VU8T94"
      },
      "outputs": [],
      "source": [
        "# Testar o aprendizado com imagens diferentes e com arquivo já Treinado com 100 Epochs\n",
        "\n",
        "diretorio = '/content/dataset/Fotos_Teste'\n",
        "\n",
        "# 1. Carregar o modelo treinado\n",
        "model = YOLO('/content/dataset/Treinado/train/weights/best.pt')\n",
        "\n",
        "testar_imagens_jpg(diretorio,model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1ugAZk1mtX2otDUwaDGd9dwe3NiSy3uGA",
      "authorship_tag": "ABX9TyMZk56MGn9QJMFEu9KGPtqp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}